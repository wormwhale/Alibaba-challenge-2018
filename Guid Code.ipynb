{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://ankivil.com/kaggle-first-steps-with-julia-chars74k-first-place-using-convolutional-neural-networks/\n",
    "\n",
    "https://github.com/erhwenkuo/deep-learning-with-keras-notebooks/blob/master/2.0-first-steps-with-julia.ipynb\n",
    "\n",
    "http://florianmuellerklein.github.io/cnn_streetview/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article, I will describe how to design a Convolutional Neural Network (CNN) with Keras to score over 0.86 accuracy in the Kaggle competition First Steps With Julia. I will explain precisely how to get to this result, from data to submission. All the python code is, of course, included. This work is inspired by Florian Muellerklein’s Using deep learning to read street signs.\n",
    "\n",
    "The goal of the Kaggle competition First Steps With Julia is to classify images of characters taken from natural images. These images come from a subset of the Chars74k data set. This competition normally serves as a tutorial on how to use the Julia language but a CNN is the tool of choice to tackle this kind of problem.\n",
    "\n",
    "http://florianmuellerklein.github.io/cnn_streetview/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'source-code-files',\n",
       " 'test',\n",
       " 'testResized',\n",
       " 'train',\n",
       " 'trainResized',\n",
       " 'Untitled.ipynb']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing: Image Color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all images in the train and test sets are color images. The first step in the preprocessing is to convert all images to grayscale. It simplifies the data fed to the network and makes it easier to generalize, a blue letter being equivalent to a red letter. This preprocessing should have almost no negative impact on the final accuracy because most texts have high contrast with their background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing: Image Resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the images have different shapes and size, we have to normalize them for the model. There are two main questions for this normalization: which size do we choose? and do we keep the aspect ratio?\n",
    "\n",
    "Initially, I thought keeping the aspect ratio would be better because it would not distort the image arbitrarily. It could also lead to confusion between O and 0 (capital o and zero). However, after some tests, it seems that the results are better without keeping the aspect ratio. Maybe my filling strategy (see the code below) is not the best one.\n",
    "\n",
    "Concerning the image size, 16×16 images allow very fast training but don’t give the best results. These small images are perfect to rapidly test ideas. Using 32×32 images makes the training quite fast and gives good accuracy. Finally, using 64×64 images makes the training quite slow and marginally improves the results compared to 32×32 images. I chose to use 32×32 images because it is the best trade-off between speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "#fn = open('trainLabels.csv', 'r')\n",
    "#train_label = [dict(i) for i in csv.DictReader(fn)]\n",
    "#for i in csv.reader(fn):\n",
    "#    print(i)\n",
    "#fn.close()\n",
    "#import pandas as pd\n",
    "#pd.DataFrame(train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing: Label Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to convert the labels from characters to one-hot vectors. This is mandatory to feed the labels information to the network. This is a two-step procedure. First, we have to find a way to convert characters to consecutive integers and back. Second, we have to convert each integer to a one-hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2int(ch):\n",
    "    asciiVal = ord(ch)\n",
    "    if(asciiVal<=57): #0-9\n",
    "        asciiVal-=48\n",
    "    elif(asciiVal<=90): #A-Z\n",
    "        asciiVal-=55\n",
    "    else: #a-z\n",
    "        asciiVal-=61\n",
    "    return asciiVal\n",
    "    \n",
    "def int2label(i):\n",
    "    if(i<=9): #0-9\n",
    "        i+=48\n",
    "    elif(i<=35): #A-Z\n",
    "        i+=55\n",
    "    else: #a-z\n",
    "        i+=61\n",
    "    return chr(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \".\"\n",
    "os.path.exists( path + \"/trainResized\" )\n",
    "if not os.path.exists( path + \"/trainResized\" ):\n",
    "    os.makedirs( path + \"/trainResized\" )\n",
    "if not os.path.exists( path + \"/testResized\" ):\n",
    "    os.makedirs( path + \"/testResized\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread, imsave\n",
    "\n",
    "#trainFiles = glob.glob( path + \"/train/*\" )\n",
    "#for i, nameFile in enumerate(trainFiles):\n",
    "\n",
    "#    image = imread( nameFile )\n",
    "#    imageResized = resize( image, (20,20) )\n",
    "#    newName = \"/\".join( nameFile.split(\"/\")[:-1] ) + \"Resized/\" + nameFile.split(\"/\")[-1]\n",
    "#    print(\"/\".join( nameFile.split(\"/\")[:-1] ) + 'Resized/' + nameFile.split(\"/\")[-1])\n",
    "#    imsave ( newName, imageResized )\n",
    "#    if i == 1:\n",
    "#        print(image.shape) # (89, 71, 3)\n",
    "#        print(imageResized.shape) # (20, 20, 3)\n",
    "\n",
    "#testFiles = glob.glob( path + \"/test/*\" )\n",
    "#for i, nameFile in enumerate(testFiles):\n",
    "#    image = imread( nameFile )\n",
    "#    imageResized = resize( image, (20,20) )\t\n",
    "#    newName = \"/\".join( nameFile.split(\"/\")[:-1] ) + \"Resized/\" + nameFile.split(\"/\")[-1]\n",
    "#    imsave ( newName, imageResized )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imsave, imresize\n",
    "from natsort import natsorted\n",
    "\n",
    "# Path of data files\n",
    "path = \".\"\n",
    "\n",
    "# Input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "\n",
    "# Keep or not the initial image aspect ratio\n",
    "keepRatio = False\n",
    "\n",
    "# Create the directories if needed\n",
    "if not os.path.exists( path + \"/trainResized\"):\n",
    "    os.makedirs(path + \"/trainResized\")\n",
    "if not os.path.exists( path + \"/testResized\"):\n",
    "    os.makedirs(path + \"/testResized\")\n",
    "    \n",
    "    \n",
    "### Images preprocessing ###\n",
    "\n",
    "for setType in [\"train\", \"test\"]:\n",
    "    # We have to make sure files are sorted according to labels, even if they don't have trailing zeros\n",
    "    files = natsorted(glob.glob(path + \"/\"+setType+\"/*\"))\n",
    "    \n",
    "    data = np.zeros((len(files), img_rows, img_cols)) #will add the channel dimension later\n",
    "    \n",
    "    for i, filepath in enumerate(files):\n",
    "        image = imread(filepath, True) #True: flatten to grayscale\n",
    "        if keepRatio:\n",
    "            # Find the largest dimension (height or width)\n",
    "            maxSize = max(image.shape[0], image.shape[1])\n",
    "            \n",
    "            # Size of the resized image, keeping aspect ratio\n",
    "            imageWidth = math.floor(img_rows*image.shape[0]/maxSize)\n",
    "            imageHeigh = math.floor(img_cols*image.shape[1]/maxSize)\n",
    "            \n",
    "            # Compute deltas to center image (should be 0 for the largest dimension)\n",
    "            dRows = (img_rows-imageWidth)//2\n",
    "            dCols = (img_cols-imageHeigh)//2\n",
    "                        \n",
    "            imageResized = np.zeros((img_rows, img_cols))\n",
    "            imageResized[dRows:dRows+imageWidth, dCols:dCols+imageHeigh] = imresize(image, (imageWidth, imageHeigh))\n",
    "            \n",
    "            # Fill the empty image with the median value of the border pixels\n",
    "            # This value should be close to the background color\n",
    "            val = np.median(np.append(imageResized[dRows,:],\n",
    "                                      (imageResized[dRows+imageWidth-1,:],\n",
    "                                      imageResized[:,dCols],\n",
    "                                      imageResized[:,dCols+imageHeigh-1])))\n",
    "                                      \n",
    "            # If rows were left blank\n",
    "            if(dRows>0):\n",
    "                imageResized[0:dRows,:].fill(val)\n",
    "                imageResized[dRows+imageWidth:,:].fill(val)\n",
    "                \n",
    "            # If columns were left blank\n",
    "            if(dCols>0):\n",
    "                imageResized[:,0:dCols].fill(val)\n",
    "                imageResized[:,dCols+imageHeigh:].fill(val)\n",
    "        else:\n",
    "            imageResized = imresize(image, (img_rows, img_cols))\n",
    "        \n",
    "        # Add the resized image to the dataset\n",
    "        data[i] = imageResized\n",
    "        \n",
    "        #Save image (mostly for visualization)\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        filenameDotSplit = filename.split(\".\")\n",
    "        newFilename = str(int(filenameDotSplit[0])).zfill(5) + \".\" + filenameDotSplit[-1].lower()  #Add trailing zeros\n",
    "        newName = \"/\".join(filepath.split(\"/\")[:-1] ) + 'Resized' + \"/\" + newFilename\n",
    "        imsave(newName, imageResized)\n",
    "        \n",
    "    # Add channel/filter dimension\n",
    "    data = data[:,:,:, np.newaxis] \n",
    "    \n",
    "    # Makes values floats between 0 and 1 (gives better results for neural nets)\n",
    "    data = data.astype('float32')\n",
    "    data /= 255\n",
    "    \n",
    "    # Save the data as numpy file for faster loading\n",
    "    np.save(path+\"/\"+setType+ 'ResizedData' +\".npy\", data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Resized images to data for the input of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from reSized images\n",
    "for i_type in ['train', 'test']:\n",
    "    files = natsorted(glob.glob('./' + i_type + 'Resized/*'))\n",
    "    data = np.zeros((len(files), img_rows, img_cols))\n",
    "\n",
    "    for i, i_path in enumerate(files):\n",
    "        data[i] = imread(i_path, True)\n",
    "    data = data[:, :, :, np.newaxis]\n",
    "    data = data.astype('float32')\n",
    "    data /= 255\n",
    "    np.save(path+\"/\"+i_type+ 'ResizedData' +\".npy\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Labels preprocessing ###\n",
    "\n",
    "# Load labels\n",
    "y_train = pd.read_csv(path+\"/trainLabels.csv\").values[:,1] #Keep only label\n",
    "\n",
    "# Convert labels to one-hot vectors\n",
    "Y_train = np.zeros((y_train.shape[0], len(np.unique(y_train))))\n",
    "\n",
    "for i in range(y_train.shape[0]):\n",
    "    Y_train[i][label2int(y_train[i])] = 1 # One-hot\n",
    "\n",
    "# Save preprocessed label to nupy file for faster loading\n",
    "np.save(path+\"/\"+\"labelsPreproc.npy\", Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the training data as it is, we can apply some augmentations to artificially increase the size of the training set with “new” images. Augmentations are random transformations applied to the initial data to produce a modified version of it. These transformations can be a zoom, a rotation, etc. or a combination of all these.\n",
    "\n",
    "https://keras.io/preprocessing/image/#imagedatagenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ImageDataGenerator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ImageDataGenerator constructor takes several parameters to define the augmentations we want to use. I will only go through the parameters useful for our case, see the documentation if you need other modifications to your images:\n",
    "\n",
    "**featurewise_center , featurewise_std_normalization and zca_whitening are not used as they don’t increase the performance of the network. If you want to test these options, be sure to compute the relevant quantities with fit and apply these modifications to your test set with standardize .\n",
    "\n",
    "**rotation_range Best results for values around 20.\n",
    "\n",
    "**width_shift_range Best results for values around 0.15.\n",
    "\n",
    "**height_shift_range Best results for values around 0.15.\n",
    "\n",
    "**shear_range Best results for values around 0.4.\n",
    "\n",
    "**zoom_range Best results for values around 0.3.\n",
    "\n",
    "**channel_shift_range Best results for values around 0.1.\n",
    "\n",
    "Of course, I didn’t test all the combinations, so there must be others values which increase the final accuracy. Be careful though, too much augmentation (high parameter values) will make the learning slow or even impossible.\n",
    "\n",
    "I also added the possibility for the ImageDataGenerator to randomly invert the values, the code is below. The parameters are:\n",
    "\n",
    "**channel_flip Best set to True.\n",
    "\n",
    "**channel_flip_max Should be set to 1. as we normalized the data between 0 and 1.\n",
    "\n",
    "\n",
    "使用 ImageDataGenerator\n",
    "ImageDataGenerator構建函數需要幾個參數來定義我們想要使用的增強效果。我只會通過對我們的案例有用的參數進行設定，如果您需要對您的圖像進行其他修改，請參閱Keras文檔。\n",
    "\n",
    "featurewise_center，featurewise_std_normalization和zca_whitening不使用，因為在本案例裡它們不會增加網絡的性能。如果你想測試這些選項，一定要合適地計算相關的數量，並將這些修改應用到你的測試集中進行標準化。\n",
    "\n",
    "rotation_range 20左右的值效果最好。\n",
    "\n",
    "width_shift_range 0.15左右的值效果最好。\n",
    "\n",
    "height_shift_range 0.15左右的值效果最好。\n",
    "\n",
    "shear_range 0.4 左右的值效果最好。\n",
    "\n",
    "zoom_range 0.3 左右的值效果最好。\n",
    "\n",
    "channel_shift_range 0.1左右的值效果最好。\n",
    "\n",
    "當然，我沒有測試所有的組合，所以可能還有其他值的組合可以用來提高最終的準確度。但要小心，太多的增量（高參數值）會使學習變得緩慢甚至跑不出來。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 模型學習 (Learning)\n",
    "\n",
    "對於模型的訓練，我使用了分類交叉熵(cross-entropy)作為損失函數(loss function)，最後一層使用softmax的激勵函數。\n",
    "\n",
    "# 演算法 (Algorithm)\n",
    "\n",
    "在這個模型裡我選擇使用AdaMax和AdaDelta來作為優化器(optimizer)，而不是使用經典的隨機梯度下降（SGD）算法。 同時我發現AdaMax比AdaDelta在這個問題上會給出更好的結果。但是，對於具有眾多濾波器和大型完全連接層的複雜網絡，AdaMax在訓練循環不太收斂，甚至無法完全收斂。因此在這次的網絡訓練過程我拆成二個階段。 第一個階段，我先使用AdaDelta進行了20個循環的前期訓練為的是要比較快速的幫忙卷積網絡的模型收斂。第二個階段，則利用AdaMax來進行更多訓練循環與更細微的修正來得到更好的模型。如果將網絡的大小除以2，則不需要使用該策略。\n",
    "\n",
    "# 訓練批次量 (Batch Size)\n",
    "在保持訓練循環次數不變的同時，我試圖改變每次訓練循環的批量大小(batch size)。大的批量(batch)會使算法運行速度更快，但結果效能不佳。 這可能是因為在相同數量的數據量下，更大的批量意味著更少的模型權重的更新。無論如何，在這個範例中最好的結果是在批量(batch size) 設成 128的情況下達到的。\n",
    "\n",
    "# 網絡層的權重初始 (Layer Initialization)\n",
    "\n",
    "如果網絡未正確初始化，則優化算法可能無法找到最佳值。我發現使用he_normal來進行初始化會使模型的學習變得更容易。在Keras中，你只需要為每一層使用kernel_initializer='he_normal'參數。\n",
    "\n",
    "# 學習率衰減 (Learning Rate Decay)\n",
    "\n",
    "在訓練期間逐漸降低學習率(learning rate)通常是一個好主意。它允許算法微調參數，並接近局部最小值。 但是，我發現使用AdaMax的optimizer，在\n",
    "沒有設定學習速率衰減的情況下結果更好，所以我們現在不必擔心。\n",
    "\n",
    "# 訓練循環 (Number of Epochs)\n",
    "\n",
    "使用128的批量大小，沒有學習速度衰減，我測試了200到500個訓練循環。即使運行到第500個訓練循環，整個網絡模型似乎也沒出現過擬合(overfitting)的情形。 我想這肯定要歸功於Dropout的設定發揮了功效。我發現500個訓練循環的結果比300個訓練循環略好。最後的模型我用了500個訓練循環，但是如果你在CPU上運行，300個訓練循環應該就足夠了。\n",
    "\n",
    "# 交叉驗證 (Cross-Validation)\n",
    "\n",
    "為了評估不同模型的質量和超參數的影響，我使用了蒙特卡洛交叉驗證：我隨機分配了初始數據1/4進行驗證，並將3/4進行學習。 我還使用分裂技術，確保在我們的例子中，每個類別約有1/4圖像出現在測試集中。這導致更穩定的驗證分數。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters for the network\n",
    "batch_size = 128 # 訓練批次量 (Batch Size)\n",
    "nb_classes = 62  # A-Z, a-z, 0-9共有62個類別\n",
    "nb_epoch = 500   # 進行500個訓練循環\n",
    "\n",
    "# Input image dimensions\n",
    "# 要輸入到第一層網絡的圖像大小 (32像素 x 32像素)\n",
    "img_height, img_width = 32, 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相關資料的路徑\n",
    "path = \".\"\n",
    "# 載入預處理好的訓練資料與標籤\n",
    "X_train_all = np.load(path+\"/trainResizedData.npy\")\n",
    "Y_train_all = np.load(path+\"/labelsPreproc.npy\")\n",
    "# 將資料區分為訓練資料集與驗證資料集\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_all, Y_train_all, test_size=0.25, stratify=np.argmax(Y_train_all, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4712, 32, 32, 1)\n",
      "(4712, 62)\n"
     ]
    }
   ],
   "source": [
    "# For each image data, what dimension does it have?\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 設定圖像增強(data augmentation)的設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.15,\n",
    "    height_shift_range = 0.15,\n",
    "    shear_range = 0.4,\n",
    "    zoom_range = 0.3,                    \n",
    "    channel_shift_range = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 128)       1280      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4096)              33558528  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 62)                254014    \n",
      "=================================================================\n",
      "Total params: 57,527,742\n",
      "Trainable params: 57,527,742\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### 卷積網絡模型架構 ###\n",
    "model = Sequential()\n",
    "\n",
    "# 25 filter, each one has size 3*3\n",
    "\n",
    "model.add(Convolution2D(128,(3, 3), padding='same', kernel_initializer='he_normal', activation='relu', \n",
    "                        input_shape=(img_height, img_width, 1)))\n",
    "\n",
    "model.add(Convolution2D(128,(3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(256,(3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(Convolution2D(256,(3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(512,(3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(Convolution2D(512,(3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(Convolution2D(512,(3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(4096, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# output; we have nb_classes. Therefore, we put this dense layer with nb_classes nodes.\n",
    "model.add(Dense(nb_classes, kernel_initializer='he_normal', activation='softmax')) \n",
    "\n",
    "# 展現整個模型架構\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we use AdaDelta to train our model.\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adadelta',  \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# We take epochs = 20.\n",
    "model.fit(X_train, Y_train, batch_size=batch_size,\n",
    "                    epochs=20, \n",
    "                    validation_data=(X_val, Y_val),\n",
    "                    verbose=1)\n",
    "# Second, we use AdaMax to train our model subsequently.\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adamax',  \n",
    "              metrics=[\"accuracy\"])\n",
    "# Here, we will save the better model with great validation during our training.\n",
    "saveBestModel = ModelCheckpoint(\"best.kerasModelWeights\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Moreover, in this training step, we will generate images from ImageDataGenrator to add our second training process.\n",
    "history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(X_train)/batch_size,\n",
    "                    epochs=nb_epoch, \n",
    "                    validation_data=(X_val, Y_val),\n",
    "                    callbacks=[saveBestModel],\n",
    "                    verbose=1)\n",
    "\n",
    "### 進行預測 ###\n",
    "\n",
    "# 載入訓練過程中驗證結果最好的模型\n",
    "model.load_weights(\"best.kerasModelWeights\")\n",
    "\n",
    "# 載入Kaggle測試資料集\n",
    "X_test = np.load(path+\"/testPreproc.npy\")\n",
    "\n",
    "# 預測字符的類別\n",
    "Y_test_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從類別的數字轉換為字符\n",
    "vInt2label = np.vectorize(int2label)\n",
    "Y_test_pred = vInt2label(Y_test_pred) \n",
    "\n",
    "# 保存預測結果到檔案系統\n",
    "np.savetxt(path+\"/jular_pred\" + \".csv\", np.c_[range(6284,len(Y_test_pred)+6284),Y_test_pred], delimiter=',', header = 'ID,Class', comments = '', fmt='%s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 透過趨勢圖來觀察訓練與驗證的走向 (特別去觀察是否有\"過擬合(overfitting)\"的現象)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 把每個訓練循環(epochs)的相關重要的監控指標取出來\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# 取得整個訓練循環(epochs)的總次數\n",
    "epochs = range(len(acc))\n",
    "\n",
    "# 把\"訓練準確率(Training acc)\"與\"驗證準確率(Validation acc)\"的趨勢線形表現在圖表上\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# 把\"訓練損失(Training loss)\"與\"驗證損失(Validation loss)\"的趨勢線形表現在圖表上\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "從\"Training與validation accuracy\"的線型圖來看, 訓練到50~60循環(epochs)之後驗證的準確率就提不上去了, 但是訓練的準確率確可以一直提高。 雖然說83%的預測準確率在Kaggle的competition裡己經是前10名左右了, 但如果想要繼續提升效果的話可的的方向:\n",
    "增加更多的字符圖像\n",
    "字符圖像的增強的調教(可以增加如原文提及的影像頻導channel的flip,在這個文章為了簡化起見移除了這個部份的實作)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
